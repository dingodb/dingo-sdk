# Copyright (c) 2023 dingodb.com, Inc. All Rights Reserved

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#!/usr/bin/env python3
"""
å‘é‡æ•°æ®ä¸“ç”¨çš„HDF5åˆ°JSONè½¬æ¢å™¨
å°†å‘é‡æ•°æ®è½¬æ¢ä¸ºid+embæ ¼å¼çš„JSONæ–‡ä»¶
embåˆ—å­˜å‚¨ä¸ºå‘é‡æ•°ç»„ï¼Œä¸C++åŸºå‡†æµ‹è¯•æ¡†æ¶å…¼å®¹
"""

import h5py
import json
import numpy as np
import time
import argparse
from pathlib import Path
import gc
from tqdm import tqdm


def convert_vector_dataset_to_json(
    hdf5_path: str,
    dataset_name: str,
    output_path: str,
    chunk_size: int = 10000,
    pretty_print: bool = False,
):
    """
    å°†å‘é‡æ•°æ®é›†è½¬æ¢ä¸ºid+embæ ¼å¼çš„JSONæ–‡ä»¶

    Args:
        hdf5_path: HDF5æ–‡ä»¶è·¯å¾„
        dataset_name: æ•°æ®é›†åç§° (å¦‚ 'train', 'test')
        output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
        chunk_size: åˆ†å—å¤§å°
        pretty_print: æ˜¯å¦æ ¼å¼åŒ–è¾“å‡ºJSON
    """
    print(f"å¼€å§‹è½¬æ¢å‘é‡æ•°æ®é›†: {dataset_name}")
    print(f"è¾“å…¥æ–‡ä»¶: {hdf5_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"åˆ†å—å¤§å°: {chunk_size}")

    start_time = time.time()

    try:
        with h5py.File(hdf5_path, "r") as f:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
            if dataset_name not in f:
                print(f"é”™è¯¯: HDF5æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°'{dataset_name}'æ•°æ®é›†")
                available_datasets = list(f.keys())
                print(f"å¯ç”¨çš„æ•°æ®é›†: {available_datasets}")
                return False

            dataset = f[dataset_name]
            print(f"{dataset_name}æ•°æ®é›†ä¿¡æ¯:")
            print(f"  å½¢çŠ¶: {dataset.shape}")
            print(f"  æ•°æ®ç±»å‹: {dataset.dtype}")
            print(f"  æ€»å¤§å°: {dataset.nbytes / 1024 / 1024:.1f} MB")

            # æ£€æŸ¥æ•°æ®é›†ç»´åº¦ - åº”è¯¥æ˜¯2D (æ ·æœ¬æ•°, å‘é‡ç»´åº¦)
            if len(dataset.shape) != 2:
                print(f"é”™è¯¯: æœŸæœ›2Då‘é‡æ•°æ®é›†ï¼Œä½†å¾—åˆ°å½¢çŠ¶: {dataset.shape}")
                return False

            total_rows, vector_dim = dataset.shape
            print(f"æ€»æ ·æœ¬æ•°: {total_rows}, å‘é‡ç»´åº¦: {vector_dim}")

            # åˆ›å»ºè¾“å‡ºç›®å½•
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)

            # å¦‚æœæ•°æ®é›†è¾ƒå°ï¼Œç›´æ¥è½¬æ¢
            if total_rows <= chunk_size:
                print("æ•°æ®é›†è¾ƒå°ï¼Œä½¿ç”¨ç›´æ¥è½¬æ¢...")
                vectors = dataset[...]

                # åˆ›å»ºJSONæ•°æ®
                json_data = []
                for i, vector in enumerate(tqdm(vectors, desc="è½¬æ¢å‘é‡")):
                    json_obj = {
                        "id": int(i),  # ä»0å¼€å§‹çš„ID
                        "emb": vector.astype(float).tolist(),  # è½¬æ¢ä¸ºPythonåˆ—è¡¨
                    }
                    json_data.append(json_obj)

                # å†™å…¥JSONæ–‡ä»¶
                with open(output_path, "w", encoding="utf-8") as json_file:
                    if pretty_print:
                        json.dump(json_data, json_file, indent=2, ensure_ascii=False)
                    else:
                        json.dump(
                            json_data,
                            json_file,
                            separators=(",", ":"),
                            ensure_ascii=False,
                        )

            else:
                print(f"ä½¿ç”¨åˆ†å—å¤„ç†ï¼Œæ€»è¡Œæ•°: {total_rows}, åˆ†å—å¤§å°: {chunk_size}")

                # è®¡ç®—åˆ†å—æ•°é‡
                num_chunks = (total_rows + chunk_size - 1) // chunk_size
                print(f"æ€»åˆ†å—æ•°: {num_chunks}")

                # æµå¼å†™å…¥JSONæ–‡ä»¶
                with open(output_path, "w", encoding="utf-8") as json_file:
                    json_file.write("[\n")  # å¼€å§‹JSONæ•°ç»„

                    for chunk_idx in range(num_chunks):
                        start_row = chunk_idx * chunk_size
                        end_row = min(start_row + chunk_size, total_rows)

                        print(
                            f"å¤„ç†åˆ†å— {chunk_idx + 1}/{num_chunks}: è¡Œ {start_row}-{end_row-1}"
                        )

                        # è¯»å–å½“å‰åˆ†å—
                        chunk_vectors = dataset[start_row:end_row, :]

                        # å¤„ç†å½“å‰åˆ†å—çš„å‘é‡
                        for i, vector in enumerate(chunk_vectors):
                            vector_id = start_row + i
                            json_obj = {
                                "id": int(vector_id),
                                "emb": vector.astype(float).tolist(),
                            }

                            # å†™å…¥JSONå¯¹è±¡
                            if pretty_print:
                                json_line = json.dumps(
                                    json_obj, indent=2, ensure_ascii=False
                                )
                                # è°ƒæ•´ç¼©è¿›ä»¥é€‚åº”æ•°ç»„æ ¼å¼
                                json_line = "\n".join(
                                    "  " + line for line in json_line.split("\n")
                                )
                            else:
                                json_line = json.dumps(
                                    json_obj, separators=(",", ":"), ensure_ascii=False
                                )
                                json_line = f"  {json_line}"

                            # é™¤äº†æœ€åä¸€ä¸ªå…ƒç´ ï¼Œéƒ½è¦åŠ é€—å·
                            if vector_id < total_rows - 1:
                                json_line += ","

                            json_file.write(json_line + "\n")

                        # å®šæœŸåƒåœ¾å›æ”¶
                        if chunk_idx % 10 == 0:
                            gc.collect()

                    json_file.write("]")  # ç»“æŸJSONæ•°ç»„

        # è½¬æ¢å®Œæˆ
        total_time = time.time() - start_time
        print(f"\nâœ… è½¬æ¢æˆåŠŸå®Œæˆ!")
        print(f"æ€»ç”¨æ—¶: {total_time:.1f}ç§’")

        # éªŒè¯è¾“å‡ºæ–‡ä»¶
        try:
            # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
            file_size = Path(output_path).stat().st_size / 1024 / 1024
            print(f"è¾“å‡ºæ–‡ä»¶å¤§å°: {file_size:.1f} MB")

            # è¯»å–å¹¶éªŒè¯JSONæ ¼å¼
            with open(output_path, "r", encoding="utf-8") as f:
                # åªè¯»å–å‰1KBæ¥éªŒè¯æ ¼å¼
                sample = f.read(1024)
                if (
                    sample.strip().startswith("[")
                    and "id" in sample
                    and "emb" in sample
                ):
                    print("JSONæ ¼å¼éªŒè¯: âœ… æ ¼å¼æ­£ç¡®")
                else:
                    print("JSONæ ¼å¼éªŒè¯: âŒ æ ¼å¼å¯èƒ½æœ‰é—®é¢˜")

            # å°è¯•åŠ è½½å®Œæ•´JSONæ¥éªŒè¯
            print("éªŒè¯JSONå®Œæ•´æ€§...")
            with open(output_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                print(f"éªŒè¯ç»“æœ: {len(data)} æ¡è®°å½•")

                if len(data) > 0:
                    print(f"\næ•°æ®é¢„è§ˆ (å‰2æ¡):")
                    for i in range(min(2, len(data))):
                        item = data[i]
                        print(
                            f"  è®°å½• {i}: ID={item['id']}, å‘é‡ç»´åº¦={len(item['emb'])}"
                        )
                        print(f"    å‘é‡å‰10ä¸ªå…ƒç´ : {item['emb'][:10]}")

        except Exception as e:
            print(f"éªŒè¯è¾“å‡ºæ–‡ä»¶æ—¶å‡ºé”™: {e}")

        return True

    except Exception as e:
        print(f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return False


def convert_neighbors_to_json(
    hdf5_path: str,
    output_path: str = "neighbors.json",
    chunk_size: int = 1000,
    pretty_print: bool = False,
    max_neighbors: int = 100,
):  # æ·»åŠ é‚»å±…æ•°é‡é™åˆ¶
    """
    ä¸“é—¨è½¬æ¢neighborsæ•°æ®é›†åˆ°JSONæ ¼å¼
    """
    print(f"å¼€å§‹è½¬æ¢neighborsæ•°æ®é›†...")
    print(f"è¾“å…¥æ–‡ä»¶: {hdf5_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"åˆ†å—å¤§å°: {chunk_size}")
    print(f"æœ€å¤§é‚»å±…æ•°: {max_neighbors}")  # æ–°å¢æ—¥å¿—

    start_time = time.time()

    try:
        with h5py.File(hdf5_path, "r") as f:
            if "neighbors" not in f:
                print("é”™è¯¯: HDF5æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°'neighbors'æ•°æ®é›†")
                return False

            neighbors_dataset = f["neighbors"]
            print(f"neighborsæ•°æ®é›†ä¿¡æ¯:")
            print(f"  å½¢çŠ¶: {neighbors_dataset.shape}")
            print(f"  æ•°æ®ç±»å‹: {neighbors_dataset.dtype}")

            total_rows, total_cols = neighbors_dataset.shape
            if total_cols < max_neighbors:
                max_neighbors = total_cols
                print(
                    f"åŸå§‹é‚»å±…æ•°: {total_cols} < max_neighbors: {max_neighbors} ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹é‚»å±…æ•°: {total_cols}"
                )
            else:
                print(f"åŸå§‹é‚»å±…æ•°: {total_cols}, é™åˆ¶ä¸º: {max_neighbors}")

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)

            if total_rows <= chunk_size:
                # å°æ•°æ®é›†ç›´æ¥è½¬æ¢
                data = neighbors_dataset[...]

                json_data = []
                for i, neighbors_row in enumerate(tqdm(data, desc="è½¬æ¢neighbors")):
                    # é™åˆ¶é‚»å±…æ•°é‡
                    limited_neighbors = (
                        neighbors_row[:max_neighbors].astype(int).tolist()
                    )
                    json_obj = {
                        "id": int(i),
                        "neighbors_id": limited_neighbors,  # ä½¿ç”¨é™åˆ¶åçš„é‚»å±…åˆ—è¡¨
                    }
                    json_data.append(json_obj)

                # å†™å…¥JSONæ–‡ä»¶
                with open(output_path, "w", encoding="utf-8") as json_file:
                    if pretty_print:
                        json.dump(json_data, json_file, indent=2, ensure_ascii=False)
                    else:
                        json.dump(
                            json_data,
                            json_file,
                            separators=(",", ":"),
                            ensure_ascii=False,
                        )

            else:
                # å¤§æ•°æ®é›†åˆ†å—å¤„ç†
                num_chunks = (total_rows + chunk_size - 1) // chunk_size
                print(f"æ€»åˆ†å—æ•°: {num_chunks}")

                with open(output_path, "w", encoding="utf-8") as json_file:
                    json_file.write("[\n")  # å¼€å§‹JSONæ•°ç»„

                    for chunk_idx in range(num_chunks):
                        start_row = chunk_idx * chunk_size
                        end_row = min(start_row + chunk_size, total_rows)

                        print(
                            f"å¤„ç†åˆ†å— {chunk_idx + 1}/{num_chunks}: è¡Œ {start_row}-{end_row-1}"
                        )

                        chunk_data = neighbors_dataset[start_row:end_row, :]

                        for i, neighbors_row in enumerate(chunk_data):
                            query_id = start_row + i
                            # é™åˆ¶é‚»å±…æ•°é‡
                            limited_neighbors = (
                                neighbors_row[:max_neighbors].astype(int).tolist()
                            )
                            json_obj = {
                                "id": int(query_id),
                                "neighbors": limited_neighbors,  # ä½¿ç”¨é™åˆ¶åçš„é‚»å±…åˆ—è¡¨
                            }

                            # å†™å…¥JSONå¯¹è±¡
                            if pretty_print:
                                json_line = json.dumps(
                                    json_obj, indent=2, ensure_ascii=False
                                )
                                json_line = "\n".join(
                                    "  " + line for line in json_line.split("\n")
                                )
                            else:
                                json_line = json.dumps(
                                    json_obj, separators=(",", ":"), ensure_ascii=False
                                )
                                json_line = f"  {json_line}"

                            # é™¤äº†æœ€åä¸€ä¸ªå…ƒç´ ï¼Œéƒ½è¦åŠ é€—å·
                            if query_id < total_rows - 1:
                                json_line += ","

                            json_file.write(json_line + "\n")

                    json_file.write("]")  # ç»“æŸJSONæ•°ç»„

        total_time = time.time() - start_time
        print(f"âœ… neighborsè½¬æ¢å®Œæˆ! ç”¨æ—¶: {total_time:.1f}ç§’")
        return True

    except Exception as e:
        print(f"è½¬æ¢neighborsæ—¶å‡ºé”™: {e}")
        return False


def parse_datasets(datasets_str: str) -> list:
    """è§£ææ•°æ®é›†å­—ç¬¦ä¸²"""
    if not datasets_str:
        return []
    return [d.strip() for d in datasets_str.split(",") if d.strip()]


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å‘é‡æ•°æ®ä¸“ç”¨HDF5åˆ°JSONè½¬æ¢å™¨")
    parser.add_argument("hdf5_file", help="è¾“å…¥çš„HDF5æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "-d",
        "--datasets",
        default="train,test,neighbors",
        help="è¦è½¬æ¢çš„å¤šä¸ªæ•°æ®é›†åç§° (é»˜è®¤: train,test,neighbors)",
    )
    parser.add_argument(
        "-o", "--outputdir", default=".", help="è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: .)"
    )
    parser.add_argument(
        "-c",
        "--chunk-size",
        type=int,
        default=10000,
        help="åˆ†å—å¤§å°ï¼Œæ§åˆ¶å†…å­˜ä½¿ç”¨ (é»˜è®¤: 10000)",
    )
    parser.add_argument(
        "--max-neighbors", type=int, default=100, help="æœ€å¤§é‚»å±…æ•°é‡ (é»˜è®¤: 100)"
    )
    parser.add_argument(
        "--pretty", action="store_true", default=False, help="æ ¼å¼åŒ–è¾“å‡ºJSON"
    )
    parser.add_argument(
        "--preview",
        action="store_true",
        default=False,
        help="é¢„è§ˆHDF5æ–‡ä»¶ä¸­çš„æ•°æ®é›†ä¿¡æ¯",
    )

    args = parser.parse_args()

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.hdf5_file).exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {args.hdf5_file}")
        return 1

    # å¦‚æœéœ€è¦é¢„è§ˆ
    if args.preview:
        print("é¢„è§ˆHDF5æ–‡ä»¶ä¸­çš„æ•°æ®é›†...")
        try:
            with h5py.File(args.hdf5_file, "r") as f:
                print(f"æ–‡ä»¶: {args.hdf5_file}")
                print("æ•°æ®é›†åˆ—è¡¨:")
                for name, dataset in f.items():
                    if isinstance(dataset, h5py.Dataset):
                        size_mb = dataset.nbytes / 1024 / 1024
                        print(
                            f"  {name}: {dataset.shape} {dataset.dtype} ({size_mb:.1f} MB)"
                        )
                print()
                return 0
        except Exception as e:
            print(f"è¯»å–HDF5æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return 1

    # è§£ææ•°æ®é›†åˆ—è¡¨
    datasets = parse_datasets(args.datasets)
    if not datasets:
        print("é”™è¯¯: æœªæŒ‡å®šæœ‰æ•ˆçš„æ•°æ®é›†")
        return 1

    all_success = True
    for dataset in datasets:
        dataset_output = Path(args.outputdir) / f"{dataset}.json"
        print(f"è½¬æ¢æ•°æ®é›† {dataset} åˆ°: {dataset_output}")

        if dataset == "neighbors":
            success = convert_neighbors_to_json(
                args.hdf5_file,
                dataset_output,
                args.chunk_size,
                args.pretty,
                args.max_neighbors,
            )
        else:
            success = convert_vector_dataset_to_json(
                args.hdf5_file, dataset, dataset_output, args.chunk_size, args.pretty
            )

        if success:
            print(f"\nğŸ‰ è½¬æ¢å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {dataset_output}")
        else:
            print(f"æ•°æ®é›† {dataset} è½¬æ¢å¤±è´¥")
            all_success = False
            return 1

    if all_success:
        print(f"\nğŸ‰ æ‰€æœ‰è½¬æ¢å®Œæˆ! è¾“å‡ºç›®å½•: {args.outputdir}")

    return 0 if all_success else 1


if __name__ == "__main__":
    exit(main())
